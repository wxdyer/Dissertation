---
title: "EMA Preprocessing (Revol et. al. 2024 Steps 1 - 4)"
author:
  - name: Walter G. Dyer, wdyer@psu.edu, Penn State University
date: "`r Sys.Date()`"
output: 
  html_document:
      toc: true
      number_sections: false
      toc_float: true
      toc_depth: 2
---

<!-- Javascript code to zoom in and out in the plots. -->s
<!-- Code from Radovan Miletić: https://stackoverflow.com/questions/56361986/zoom-function-in-rmarkdown-html-plot -->
<script src = "https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>
<script type="text/javascript">
$(document).ready(function() {
    $('body').prepend('<div class=\"zoomDiv\"><img src=\"\" class=\"zoomImg\"></div>');
    // onClick function for all plots (img's)
    $('img:not(.zoomImg)').click(function() {
    $('.zoomImg').attr('src', $(this).attr('src')).css({width: '100%'});
    $('.zoomDiv').css({opacity: '1', width: 'auto', border: '1px solid white', borderRadius: '5px', position: 'fixed', top: '50%', left: '50%', marginRight: '-50%', transform: 'translate(-50%, -50%)', boxShadow: '0px 0px 50px #888888', zIndex: '50', overflow: 'auto', maxHeight: '100%'});
    });
    // onClick function for zoomImg
    $('img.zoomImg').click(function() {
    $('.zoomDiv').css({opacity: '0', width: '0%'}); 
    });
});
</script>


<!--------- INSTRUCTIONS ----------
This template would help you to structure and share your preprocessing. 
It is based on the ESM preprocessing framework described in Revol and al. (under review) 
In addition, it is associated with an R package, the esmtools package (https://package.esmtools.com), 
and a website to provide tutorials and R code for each step: https://preprocess.esmtools.com.
Please, follow the instructions commented and add descriptions above each chunk you create. 
This advanced version of the ESM preprocessing report provides instructions on creating an interactive document 
that mainly use esmtools functions (e.g., buttons) to enhanced user experience. 
Note that the commented instructions are not displayed in the rendered document. 
<!--------------------------------->


<!-- 
Txt() and button() css style:
When importing 'esmtools' packages, it comes with CSS styles for class ('esm-issue', 'esm-inspect', 'esm-mod') used by the txt() and button() functions. 
Customize them with CSS code in the provided style tag. 
Modify fonts, colors, etc., as you like (an example is commented).
In certain cases, you might need to use the '!important' declaration to override the default style definitions.
-->
<style>
.esm-issue{
  /* font-family: Georgia; */
}
</style>

<!-- This first chunk define the global settings-->
```{r, include=FALSE}
# Setup global settings
# knitr::opts_chunk$set(message=FALSE, warning=FALSE)
```

# Overview and data collection procedure

This R Markdown file is based on Revol et al. (2024)'s step-by-step guidelines for preprocessing ESM data: [https://preprocess.esmtools.com](https://preprocess.esmtools.com). It includes code adapted from Steps 1 - 5, tailored to preprocess EMA data for a secondary analysis of EMA data from the DVAL project (https://osf.io/nb9ct/overview?view_only=4f777b3907bb47aab6387aaa4353285c. In Part 2 of the study, participants received semi-random prompts 6 times throughout the day within predeterimined 2-hr intervals for three consecutive days. 

---
# Load packages

Import the packages:
```{r, message=FALSE, warning=FALSE}

#Data management
library(dplyr)
library(tidyr)
library(data.table)
library(here)
library(readr)   
library(readxl)
library(esmtools) # For button(), txt() functions

# Descriptive statistics
library(skimr)

# Missing values inspection
library(naniar)
library(visdat)

# Plotting
library(ggplot2)

# Character manipulation
library(stringr)
library(janitor)

# Dates and times
library(lubridate)
library(hms)
```

# Step 1: Import data and preliminary preprocessing

This section is dedicated to the first look at the data, the merging of data sources the first basic preprocessing methods (e.g., duplicates, branching items check), and checking the variable consistency when the data has just been imported.

Import the data: 
```{r}
#Define path to EMA Part 2 data
part2_path <- here("data", "DVAL_EMA_Part2_beeped.csv")
```

Raw dataset meta-info:
```{r}
dataInfo(file_path = part2_path, 
         read_fun = read.csv,
         idvar = "participant_id", 
         timevar = "init_time")
```
# Import data and prepare for preprocessing steps

Below are several steps that will facilitate the application of Revol et al's preprocessing steps. Specifically, only variables and participants relevant to the study will be included in the preprocessing steps, so the data must be subset. Additionally, how to define missing values is specified, variables are formatted to their respective types (e.g., numeric, character), and formatting issues related to cases where participants opted out of a survey are dealt with below. 

## Modification text:data are subsetted to include only participants who completed Part2 (3 days of EMA data collection
```{r}
# Define subset of participants who completed Part2 (3 days of EMA data collection)
subset_ids <- c(
  102, 104, 107, 112, 113, 115, 116, 124, 125, 126, 127, 128, 133, 141, 
  150, 151, 154, 159, 163, 164, 166, 169, 175, 177, 180, 186, 188, 192, 
  195, 196, 197, 198, 199, 202, 206, 209, 210, 213, 214, 216, 218, 221, 
  222, 223, 233, 238, 240, 246, 252, 257, 262, 268, 273, 274, 278, 281, 
  282, 286, 287, 290, 291, 295, 297, 300, 301, 304, 310, 314, 320, 321, 
  329, 332, 340, 341, 342, 344, 346, 349, 351, 356, 362, 369, 370, 
  372, 381, 383, 393, 394, 399, 416, 423, 427, 428, 430
)
```

## Issue text: The raw dataset is quite large and includes several variables irrelevant to this study. Additionally, missinginess is inconsistently coded upon data import.
## Modification text: EMA variables relevant to the study are extracted, and missingness is standardized across variables.

## Issue text: The variable smoked_since_last_survey gave participants the option to select '6+' which, while useful for other analyses, prevents us from formatting the variable as numeric.
## Modification text: The chunk below also converts '6+' to 6 for the variable smoked_since_last_survey.

## Issue text: Instances where participants opted out were reported by the literal text 'opted out' in the raw dataset, making the data incompatible with the command to format the variable as numeric.
## Modification text: In order to maintain these data but in a format compatible with numeric formatting, a variable to track opted out instances is created to check whether further actions to account for these cases must be taken.

## Issue text: Variables are all in character format upon import.
## Modification text: The below chunk also formats numeric variables as numeric.")`

Note: the below code also includes counts of missingness before and after the data modifications. No data were lost during the modification process.

```{r}
# Read in CSV 
part2beeped_raw <- read_csv(
  part2_path,
  na = c("-99", "__NA__", "NA", ""),
  col_types = cols(.default = col_character()),
  col_select = c(
    mood,
    smoke,
    before_survey,
    unpleasant,
    prior_mood,
    smoked_since_last_survey,
    pleasant,
    participant_id,
    session_id,
    completion_time,
    init_time
  )
) %>%
  filter(as.numeric(participant_id) %in% subset_ids)
```

```{r}
# Check frequency of "opted out"
vars_to_check <- c(
  "mood", "smoke", "unpleasant", "pleasant",
  "prior_mood", "smoked_since_last_survey",
  "participant_id", "session_id"
)

opted_out_counts <- part2beeped_raw %>%
  summarise(across(
    all_of(vars_to_check),
    ~ sum(. == "opted out", na.rm = TRUE)
  )) %>%
  tidyr::pivot_longer(
    everything(),
    names_to = "variable",
    values_to = "n_opted_out"
  ) %>%
  arrange(desc(n_opted_out))

opted_out_counts
```

```{r}
# Record missingness BEFORE conversion
missing_before <- part2beeped_raw %>%
  summarise(across(everything(), ~ sum(is.na(.)))) %>%
  tidyr::pivot_longer(
    everything(),
    names_to = "variable",
    values_to = "missing_before"
  )
```

# Prepare numeric variables for recoding and recode numeric variables

```{r}
part2beeped <- part2beeped_raw %>%
  mutate(
    across(
      c(smoked_since_last_survey, mood, smoke, unpleasant,
        pleasant, prior_mood, participant_id, session_id),
      ~ trimws(.)
    ),
    
#Recode smoked_since_last_survey
    smoked_since_last_survey = case_when(
      smoked_since_last_survey == "6+" ~ "6",  # convert 6+ to 6
      TRUE ~ smoked_since_last_survey
    ),
    smoked_since_last_survey = suppressWarnings(
      as.numeric(smoked_since_last_survey)
    ),
    
#Recode mood, addressing the opted out case
    mood = ifelse(mood == "opted out", NA, mood),
    mood = suppressWarnings(as.numeric(mood)),
    
#Format all other numeric variables
    smoke = suppressWarnings(as.numeric(smoke)),
    unpleasant = suppressWarnings(as.numeric(unpleasant)),
    pleasant = suppressWarnings(as.numeric(pleasant)),
    prior_mood = suppressWarnings(as.numeric(prior_mood)),
    participant_id = suppressWarnings(as.numeric(participant_id)),
    session_id = suppressWarnings(as.numeric(session_id))
    
  )
```

```{r}
# Record missingness AFTER conversion
missing_after <- part2beeped %>%
  summarise(across(everything(), ~ sum(is.na(.)))) %>%
  tidyr::pivot_longer(
    everything(),
    names_to = "variable",
    values_to = "missing_after"
  )
```

# Inspect
```{r}
missing_summary <- missing_before %>%
  left_join(missing_after, by = "variable") %>%
  arrange(variable)

missing_summary
```
Missingness increased by one case for 'mood' when the opted out data were converted to NA. Because the opportunity to examine this aspect of response behavior was nonexistent in this sample, the decision was made to simply recode the data to missing (i.e., NA).

# Modification text: format time variables as POSIXct."
```{r}
part2beeped <- part2beeped %>%
  mutate(init_time = as.POSIXct(init_time, tz = "UTC"),
         completion_time = as.POSIXct(completion_time, tz = "UTC"))
```

Cleaned dataset meta-info:
```{r}
# Basic info
cat("## Path      : in-memory dataframe\n")
cat("## Extension : RDS\n")
cat("## Size      :", format(object.size(part2beeped), units = "auto"), "\n")

# Dimensions
cat("## ncol      :", ncol(part2beeped), "\n")
cat("## nrow      :", nrow(part2beeped), "\n")

# Number of participants
id_var <- "participant_id"
n_participants <- n_distinct(part2beeped[[id_var]])
cat("## Number participants :", n_participants, "\n")

# Average number of observations per participant
avg_obs <- nrow(part2beeped) / n_participants
cat("## Average number obs :", round(avg_obs, 1), "\n")

# Time period
time_var <- "init_time"
time_min <- min(part2beeped[[time_var]], na.rm = TRUE)
time_max <- max(part2beeped[[time_var]], na.rm = TRUE)
cat("## Period : from", format(time_min, "%Y-%m-%d %H:%M:%S"), 
    "to", format(time_max, "%Y-%m-%d %H:%M:%S"), "\n")

# Variable names
cat("## Variables :", paste(names(part2beeped), collapse = ", "), "\n")
```
The EMA data are now prepared for Revol et. al.'s preprocessing workflow but must be merged with the parent study's baseline dataset for time-invariant variables (e.g., covariates). The below code 

#Modification text: Baseline variables are preprocessed according to survey author guidelines (see parent study online repository for full list of baseline measures) and planned analyses.
```{r}
# Define paths to input data
Apath <- here("data", "QuestionnairesA.xlsx")
Bpath <- here("data", "QuestionnairesB.xlsx")

# Read files into dataframes
A <- read_excel(Apath)
B <- read_excel(Bpath)

# Merge the baseline datasets
baseline <- merge(A, B, by = "participant-id", all = TRUE, suffixes = c(".A", ".B"))

# Clean column names (lowercase, underscores instead of hyphens)
baseline <- clean_names(baseline)

# Filter to participants who completed Part 2 EMA
analytic_sample <- c(102, 104, 107, 112, 113, 115, 116, 124, 125, 126, 127, 128, 133, 141, 150, 151, 154, 159, 163, 164, 166, 169, 175, 177, 180, 186, 188,
192, 195, 196, 197, 198, 199, 202, 206, 209, 210, 213, 214, 216, 218, 221, 222, 223, 233, 238, 240, 246, 252, 257, 262, 268, 273, 274, 278, 281, 282, 286, 287, 290, 291, 295, 297, 300, 301, 304, 310, 314, 320, 321, 329, 332, 340, 341, 342, 344, 346, 349, 351, 356, 362, 369, 370, 372, 381, 383, 393, 394, 399, 416, 423, 427, 428, 430)
baseline <- baseline %>% filter(participant_id %in% analytic_sample)

# ---- FTND: Fagerstrom ----
baseline <- baseline %>%
  rename(cigsperday = ftnd_4) %>%
  mutate(ftnd_4 = case_when(
    cigsperday <= 10 ~ 1,
    cigsperday > 10 & cigsperday <= 20 ~ 2,
    cigsperday > 20 & cigsperday <= 30 ~ 3,
    cigsperday > 30 ~ 4
  )) %>%
  mutate(ftnd_sum = ftnd_1 + ftnd_2 + ftnd_3 + ftnd_4 + ftnd_5 + ftnd_6)

# ---- PANAS: Positive and Negative Affect Schedule ----
baseline <- baseline %>%
  mutate(
    panas_pos = q64_pan_1 + q64_pan_3 + q64_pan_5 + q64_pan_9 + q64_pan_10 + q64_pan_12 + q64_pan_14 + q64_pan_16 + q64_pan_17 + q64_pan_19,
    panas_neg = q64_pan_2 + q64_pan_4 + q64_pan_6 + q64_pan_7 + q64_pan_8 + q64_pan_11 + q64_pan_13 + q64_pan_15 + q64_pan_18 + q64_pan_20
  )

# ---- SHAPS: Snaith-Hamilton Pleasure Scale ----
baseline <- baseline %>%
  mutate(across(starts_with("q14_sha_"), ~ ifelse(. %in% c(1,2), 1, 0), .names = "{.col}_sim")) %>%
  mutate(shaps_sim_sum = q14_sha_1_sim + q14_sha_2_sim + q14_sha_3_sim + q14_sha_4_sim + q14_sha_5_sim + q14_sha_6_sim + 
           q14_sha_7_sim + q14_sha_8_sim + q14_sha_9_sim + q14_sha_10_sim + q14_sha_11_sim + q14_sha_12_sim + 
           q14_sha_13_sim + q14_sha_14_sim)

# ---- Reformat column types ----
change_column_types <- function(df) {
  for (col_name in colnames(df)) {
    col_data <- df[[col_name]]
    if (is.numeric(col_data) || inherits(col_data, "POSIXct")) next
    if (is.character(col_data)) {
      trimmed <- trimws(col_data)
      numeric_check <- suppressWarnings(!is.na(as.numeric(trimmed)))
      if (all(numeric_check | is.na(trimmed))) {
        df[[col_name]] <- as.integer(trimmed)
      } else {
        df[[col_name]] <- trimmed
      }
    }
  }
  # Convert date columns
  for (col_name in colnames(df)) {
    if (grepl("_date_", col_name)) {
      df[[col_name]] <- as.POSIXct(df[[col_name]], format = "%m/%d/%Y")
    }
  }
  return(df)
}
baseline <- change_column_types(baseline)

# ---- Fix column name typo ----
baseline <- baseline %>% rename(total_household_income = total_houselhold_income)

# ---- Subset to analysis-relevant variables ----
baseline_clean <- baseline %>%
  select(
    participant_id,
    race,
    age,
    gender,
    total_household_income,
    bank_balance,
    ftnd_sum,
    cigsperday,
    shaps_sim_sum,
    panas_pos,
    panas_neg
  )
```
'baseline_clean' is the dataframe with all baseline variables, now prepared for merging with the EMA data.

##Issue text: the EMA data alone do not contain all participant data needed for analyses. They must be combined with baseline data.
##Modification text: The EMA and Baseline Data are merged, and baseline variables listed after the EMA variables.

```{r}
# Specify baseline variables
baseline_vars <- c(
  "race",
  "age",
  "gender",
  "total_household_income",
  "bank_balance",
  "ftnd_sum",
  "cigsperday",
  "shaps_sim_sum",
  "panas_pos",
  "panas_neg"
)

# Merge and reorder in a single step
study_sample <- part2beeped %>%
  left_join(baseline_clean, by = "participant_id") %>%
  select(
    # All columns except the baseline variables first
    -all_of(baseline_vars),
    # Then baseline variables at the end
    all_of(baseline_vars)
  )

# Optional: inspect the first few rows and column order
head(study_sample)
colnames(study_sample)
```

study_sample meta-info:

```{r}
cat("## Path      : in-memory dataframe\n")
cat("## Extension : RDS (original format, now in-memory)\n")
cat("## Size      :", format(object.size(study_sample), units = "auto"), "\n\n")

# Dimensions
cat("## ncol      :", ncol(study_sample), "\n")
cat("## nrow      :", nrow(study_sample), "\n\n")

# Number of participants
id_var <- "participant_id"
n_participants <- n_distinct(study_sample[[id_var]])
cat("## Number of participants :", n_participants, "\n")

# Average number of observations per participant
avg_obs <- nrow(study_sample) / n_participants
cat("## Average number of observations per participant :", round(avg_obs, 1), "\n\n")

# Time period
time_var <- "init_time"
if(time_var %in% names(study_sample)) {
  time_min <- min(study_sample[[time_var]], na.rm = TRUE)
  time_max <- max(study_sample[[time_var]], na.rm = TRUE)
  cat("## Period : from", format(time_min, "%Y-%m-%d %H:%M:%S"),
      "to", format(time_max, "%Y-%m-%d %H:%M:%S"), "\n\n")
} else {
  cat("## Period : init_time variable not found in dataset\n\n")
}

# Variable names
cat("## Variables :", paste(names(study_sample), collapse = ", "), "\n")
```


## First glimpse

The following chunk helps to have a first insight on what the dataset looks like.

```{r}
dim(study_sample)
```
```{r}
head(study_sample)
```

```{r}
tail(study_sample)
```
```{r}
glimpse(study_sample)
```
```{r}
summary(study_sample)
```
```{r}
table(study_sample$before_survey, useNA="ifany")
```
#Revol and colleagues' (2024) step for renaming and relabeling step skipped here because it was completed in previous steps.

## Duplication
No issues detected.

**Look for duplicated rows:** the dataframe does not have duplicated rows.
```{r}
sum(duplicated(study_sample))
```

**Look for duplicated answers** (duplication in the self-reported items): no issue found regarding the number of self-reported items, the values of the variables involved, and the number of times duplicated .

```{r}
study_sample[!is.na(study_sample$init_time),] %>% # Select answered observations
    select(mood:pleasant) %>%  # Select self-report items
    group_by_all() %>%
    summarise(n = n()) %>% # Compute number of similar self-reported item values
    filter(n > 1) %>% arrange(desc(n)) %>% as.data.frame()
```


```{r}
# Check for duplicated answers based on key variables
answer_cols <- c("participant_id", "mood", "smoke", "before_survey", 
                 "unpleasant", "prior_mood", "smoked_since_last_survey", 
                 "pleasant", "session_id", "completion_time", "init_time")

dup_answers <- study_sample %>%
  group_by(across(all_of(answer_cols))) %>%
  filter(n() > 1) %>%
  ungroup()

cat("Duplicated answer sets:", nrow(dup_answers), "\n")
```

## Branching items

**Branching items are consistent:**

- when the participant chooses 'It was generally pleasant' for the variable 'before_survey', the variable  'pleasant' is displayed to the participant.
- when the participant chooses 'It was generally unpleasant' for the variable 'before_survey', the variable  'unpleasant' is displayed to the participant.
- n of observations for which both 'unpleasant' and 'pleasant' are missing (N=648) correspond to the n of observations for which participants reported 'It was generally neutral' (N=549; above) plus the n missinenss for the 'before_survey' variable (N=99; above), indicating no problems occurred at this step.

```{r}
# Count the number of observations for each before_survey category
branch_check <- study_sample %>%
  mutate(
    # Flag cases where both numeric branching variables are missing
    both_missing = is.na(pleasant) & is.na(unpleasant)
  ) %>%
  summarise(
    n_pleasant_displayed = sum(!is.na(pleasant) & before_survey == "It was generally pleasant"),
    n_unpleasant_displayed = sum(!is.na(unpleasant) & before_survey == "It was generally unpleasant"),
    n_neutral_branch = sum(both_missing & before_survey == "It was generally neutral"),
    n_both_missing_total = sum(both_missing),
    total_obs = n()
  )

branch_check
```
# Check variable coherence
# **Consistency of time-invariant variables.**
```{r}
study_sample %>%
  group_by(participant_id) %>%
  summarise(n_age_values = n_distinct(age)) %>%
  filter(n_age_values > 1)

study_sample %>%
  group_by(participant_id) %>%
  summarise(n_race_values = n_distinct(race)) %>%
  filter(n_race_values > 1)

study_sample %>%
  group_by(participant_id) %>%
  summarise(n_gender_values = n_distinct(gender)) %>%
  filter(n_gender_values > 1)

study_sample %>%
  group_by(participant_id) %>%
  summarise(n_total_household_income_values = n_distinct(total_household_income)) %>%
  filter(n_total_household_income_values > 1)

study_sample %>%
  group_by(participant_id) %>%
  summarise(n_panas_pos_values = n_distinct(panas_pos)) %>%
  filter(n_panas_pos_values > 1)

study_sample %>%
  group_by(participant_id) %>%
  summarise(n_panas_neg_values = n_distinct(panas_neg)) %>%
  filter(n_panas_neg_values > 1)

study_sample %>%
  group_by(participant_id) %>%
  summarise(n_bank_balance_values = n_distinct(bank_balance)) %>%
  filter(n_bank_balance_values > 1)

study_sample %>%
  group_by(participant_id) %>%
  summarise(n_ftnd_sum_values = n_distinct(ftnd_sum)) %>%
  filter(n_ftnd_sum_values > 1)

study_sample %>%
  group_by(participant_id) %>%
  summarise(n_cigsperday_values = n_distinct(cigsperday)) %>%
  filter(n_cigsperday_values > 1)
```
No issues detected.

## Further analysis of missing values

Here, our goal is to further characterize the patterns of missing values and address any preliminary issues related to them.
The following plot gives an overview of the missing values in the dataframe.


#### Overview of missing values

```{r, fig.width=7, fig.height=7}
# Reorder variables
data = study_sample %>% arrange(participant_id, completion_time)

# Overiew of missing values
vis_miss(study_sample)
```
We can see multiple patterns and some missingness among the EMA variables.

# Skipped Revol & colleagues' (2024)step to check coherence of missing values in the variables of interest because what would go here -- the missingness pattern between 'before_survey', 'pleasant', and 'unpleasant' were already examined above. 

# Create time variables
```{r}
study_sample <- study_sample %>%
  
  arrange(participant_id, completion_time) %>%
  
  group_by(participant_id) %>%
  
  mutate(
    # Time components
    year   = year(completion_time),
    month  = month(completion_time),
    day    = day(completion_time),
    hour   = hour(completion_time),
    minute = minute(completion_time),
    
    # Observation number within participant
    obsno = row_number(),
    
    # Day number since first beep
    dayno = as.numeric(
      difftime(
        as.Date(completion_time),
        as.Date(min(completion_time, na.rm = TRUE)),
        units = "days"
      )
    ) + 1,
    
    # Study duration in days
    duration = as.numeric(
      difftime(
        as.Date(max(completion_time, na.rm = TRUE)),
        as.Date(min(completion_time, na.rm = TRUE)),
        units = "days"
      )
    ) + 1
  ) %>%
  
  ungroup() %>%
  
  # Beep number within day
  group_by(participant_id, dayno) %>%
  mutate(beepno = row_number()) %>%
  ungroup()

```

#Flag (in)valid observations

#Modification text: using logical tests, we define which observations are valid. The 'valid' observations are the ones where there are no missing values in the variables of interest (i.e., 'mood', 'before_survey', 'smoke'), and two others ('pleasant', 'unpleasant') in function of the branching items (see above). We created a function that can be used later.

```{r}
# Function to flag valid observations
check_validity <- function(df) {
  
  # Check core variables are not missing
  mood_ok          <- !is.na(df$mood)
  before_survey_ok <- !is.na(df$before_survey)
  smoke_ok         <- !is.na(df$smoke)
  
  # Branching variable condition: at least one of the branching variables is non-missing
  # For 'pleasant' or 'unpleasant' depending on what should have been displayed
  branch_ok <- ifelse(
    df$before_survey == "It was generally pleasant", !is.na(df$pleasant),
    ifelse(df$before_survey == "It was generally unpleasant", !is.na(df$unpleasant), TRUE)
  )
  
  # Combine all conditions
  is_valid <- mood_ok & before_survey_ok & smoke_ok & branch_ok
  
  # Convert to integer (1 = valid, 0 = invalid)
  as.integer(is_valid)
}

# Apply function to study_sample and create 'valid' column
study_sample$valid <- check_validity(study_sample)

# Quick check
table(study_sample$valid)
```

# Step 2: Design and sample scheme checking

This section is dedicated to checking and solving issues due to inconsistencies between the planned and the actual design of the study.

## Calendar

Overview when the beeps were completed in a calendar format

```{r}
study_sample %>%
  filter(!is.na(completion_time)) %>%    
  mutate(
    year = year(completion_time),
    month = month(completion_time),
    day = day(completion_time)
  ) %>%
  group_by(year, month, day) %>%
  summarise(n_beeps = n(), .groups = "drop") %>%
  ggplot(aes(x = month, y = day, fill = n_beeps)) +
  geom_tile(color = "grey80") +
  scale_x_continuous(breaks = 1:12, labels = month.abb) +
  facet_wrap(~year, ncol = 2) +
  scale_y_reverse() +
  labs(
    title = "Calendar of Beeps Across 4 Years of Data Collection",
    x = "Month",
    y = "Day",
    fill = "N Beeps"
  ) +
  theme_minimal() +
  theme(
    strip.text = element_text(size = 10),
    axis.text.x = element_text(angle = 45, hjust = 1)
  )
```

## Sampling scheme plot and quantity of completed beeps
We proceed to the checking of the actual sample scheme and compare it to the defined one. 
```{r, warning=FALSE, message=FALSE}
study_sample %>%
  filter(!is.na(completion_time)) %>%
  mutate(
    weekday_type = ifelse(wday(completion_time, week_start = 1) %in% c(6,7), "weekend", "weekday")
  ) %>%
  group_by(participant_id, dayno, weekday_type) %>%
  summarise(n_beeps = n(), .groups = "drop") %>%
  ggplot(aes(x = factor(participant_id), y = factor(dayno))) +
  geom_point(aes(color = factor(n_beeps), shape = weekday_type), size = 1.5) +  # smaller points
  scale_color_brewer(palette = "Set2") +
  theme_minimal(base_size = 8) +  # smaller base text size
  theme(
    axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1, size = 6),
    axis.text.y = element_text(size = 6),
    legend.text = element_text(size = 7),
    legend.title = element_text(size = 8)
  ) +
  labs(
    title = "Sampling scheme plot of completed EMA beeps",
    x = "Participant ID",
    y = "Cumulative Day",
    color = "Number of completed beeps",
    shape = "Day type"
  ) 
```

Here we can see that week(end) days are unevenly distributed across participants, that participants who skip a survey one day tend to skip a survey on another day, and that compliance is relatively high across participants. 

```{r}
# Examine number of completed beeps per participant
study_sample %>%
  group_by(participant_id) %>% 
  summarize(n_completed_beeps = n(), .groups = "drop") %>%
  ggplot(aes(x = factor(participant_id), y = n_completed_beeps)) +
    geom_col(position = "dodge", fill = "steelblue") +
    scale_y_continuous(breaks = seq(0, max(study_sample$obsno, na.rm = TRUE), 5)) +
    labs(
      title = "Number of completed beeps per participant",
      x = "Participant ID",
      y = "Completed beeps"
    ) +
    theme_minimal(base_size = 10) +  # smaller base font size
    theme(
      axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1),
      plot.title = element_text(face = "bold", size = 12)
    )
```
## Coherence timestamps

We check whether there is timestamp incoherence within observations (e.g., an observation with 'start' time after the 'end' time) or between observations (e.g., an observation that was scheduled after another one but with a 'start' time that is before).
```{r}
# Select and order relevant variables
df <- study_sample[, c("participant_id", "obsno",
                       "init_time", "completion_time")]

df <- df[order(df$participant_id, df$obsno), ]


# Timestamp coherence within observations

df %>%
  group_by(participant_id) %>%
  mutate(
    end_before_start = completion_time < init_time
  ) %>%
  filter(end_before_start) %>%
  ungroup() %>%
  as.data.frame()


# Timestamp coherence between observations

df %>%
  filter(!is.na(init_time), !is.na(completion_time)) %>%
  group_by(participant_id) %>%
  mutate(
    prev_completion = lag(completion_time),
    overlap_issue = prev_completion > init_time
  ) %>%
  filter(overlap_issue) %>%
  ungroup() %>%
  as.data.frame()

```
No issues detected.

## Revol et. al.'s (2024) steps to examine time and delay to send steps were not possible with our dataset, given we lack a 'sent' variable. These steps were designed to determine whether the survey sending timing adhered to the sampling scheme, and the tests that are feasible in our dataset indicate the sampling scheme was adhered to. 

# Step 3: Participants' response behaviors

This section is dedicated to investigating how well participants engaged with the ESM study looking particularly for problematic patterns of behaviors (e.g., invalid observations, response time, careless responding).

## Sampling scheme, quantity of beeps and time started

We check how well participants followed the sampling scheme.

No issues detected.

```{r}
# Sampling Scheme plot: this plot illustrates the start times of participants’ valid observations. The x-axis represents the observation number and the y-axis shows the distribution across weekdays and weekends. We see that participant 238 only produces 10/18 valid observations and that occurrences of missing valid observations tend to exist toward the end of the observation period. 
study_sample %>%
  filter(valid == 1) %>%
  mutate(
    weekday = ifelse(
      wday(init_time, week_start = 1) %in% c(6, 7),
      "weekend",
      "weekday"
    )
  ) %>%
  ggplot(aes(x = obsno, y = factor(participant_id))) +
  geom_point(aes(color = weekday), size = 1) +
  labs(
    title = "Sampling scheme of the valid observations",
    y = "Participant id",
    x = "Observation number"
  )
```

```{r}
# Sampling scheme plot: this plot illustrates the start times of participants’ valid observations with continuous x-axis over the days of participation.

df <- study_sample %>%
  filter(valid == 1) %>%
  group_by(participant_id) %>%
  mutate(
    start_date = as.Date(min(init_time, na.rm = TRUE))
  ) %>%
  ungroup() %>%
  mutate(
    continuoustime = as.numeric(
      difftime(init_time, start_date, units = "mins")
    )
  )

# Number of study days
max_day <- ceiling(max(df$continuoustime, na.rm = TRUE) / 1440)

# Day boundaries (for vertical lines)
day_boundaries <- seq(0, max_day * 1440, by = 1440)

# Midpoints of each day (for centered labels)
day_midpoints <- day_boundaries[-length(day_boundaries)] + 720

# Labels
day_labels <- paste0(seq_len(length(day_midpoints)))

df %>%
  ggplot(aes(x = continuoustime, y = factor(participant_id))) +
  geom_point(size = 0.8) +
  scale_x_continuous(
    breaks = day_midpoints,
    labels = day_labels
  ) +
  geom_vline(xintercept = day_boundaries) +
  labs(
    title = "Continuous sampling scheme of the valid observations",
    y = "Participant_id",
    x = "Day number"
  )
```


```{r}
# Number of valid response over 'obsno' and it shows a slight decrease over time.
study_sample %>% 
  filter(valid==1) %>% 
  group_by(obsno) %>% summarize(n = n()) %>%
  ggplot(aes(x=obsno,y=n)) +
      geom_col(position = "dodge") +
        labs(title="Number of valid response over obsno",
             y="Number of valid beeps",x="Observation number")
```
```{r}
# Time of day beeps were started and (and valid).
study_sample %>%
  filter(valid == 1) %>%
  mutate(
    weekday = ifelse(
      wday(init_time, week_start = 1) %in% c(6, 7),
      "weekend",
      "weekday"
    ),
    time_of_day = hms::as_hms(init_time)
  ) %>%
  ggplot(aes(x = time_of_day)) +
  geom_histogram(bins = 100) +
  scale_x_time(breaks = scales::date_breaks("1 hour")) +
  facet_grid(weekday ~ .) +
  labs(
    title = "Time of day the beeps were started (and valid)",
    y = "Quantity of beeps started and valid",
    x = "Time of day"
  ) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

## Number of interactions
**Hour at which the beeps were started (in blue) and missed (in red):**
```{r}
df_interact <- study_sample %>%
  mutate(
    hour = hour(init_time),
    sent_ = !is.na(init_time),
    weekday = ifelse(wday(init_time, week_start = 1) %in% c(6, 7), "weekend", "weekday")
  ) %>%
  group_by(hour, weekday) %>%
  summarise(
    interact = sum(valid),
    open = sum(sent_)
  ) %>%
  mutate(not_interact = open - interact) %>%
  tidyr::pivot_longer(cols = c(interact, not_interact), names_to = "type", values_to = "value")

df_interact %>%
  mutate(type = factor(type, levels = c("not_interact", "interact"))) %>%
  ggplot(aes(x = factor(hour), y = value, fill = type)) +
  geom_col(position = "stack") +
  facet_grid(weekday ~ .) +
  labs(
    title = "Number of interactions and non-interactions with the questionnaire by hour",
    y = "Number of beeps",
    x = "Hour of day",
    fill = "Interaction"
  )
```
## Delays

Compute delay to fill  variables. Revol & colleagues' (2024) preprocessing steps for computing delay to fill is not possible because we lack a 'sent' variable. Will be only used for the creation of plot and won't be exported in the preprocessed dataframe. We see most of the valid surveys are submitted in under five minutes, which is expected. 
```{r}
study_sample <- study_sample %>%
  mutate(
    daily_end_min   = as.numeric(difftime(completion_time, init_time, units = "mins"))
  )

study_sample %>%
  filter(valid == 1 & !is.na(daily_end_min)) %>%
  ggplot(aes(x = daily_end_min)) +
    geom_histogram(bins = 100, fill = "steelblue", color = "black") +
    labs(
      title = "Histogram of questionnaire durations (valid beeps)",
      y = "Number of beeps",
      x = "Duration in minutes"
    )
```
#### Interval 2 beeps

No issues detected.
```{r}
# Compute time intervals between consecutive beeps within a day
study_sample_intervals <- study_sample %>%
  filter(valid == 1) %>% 
  arrange(participant_id, obsno) %>%
  group_by(participant_id, dayno) %>%
  mutate(time_int = as.numeric(difftime(lead(init_time), init_time, units = "mins"))) %>%
  ungroup()

# Plot histogram of intra-day beep intervals
study_sample_intervals %>%
  filter(!is.na(time_int)) %>%  # remove last beep in each day (no next beep)
  ggplot(aes(x = time_int)) +
    geom_histogram(bins = 100, fill = "steelblue", color = "black") +
    labs(
      title = "Histogram of delays between two subsequent beeps within a day",
      y = "Number of beeps",
      x = "Delay (minutes)"
    )
```
## Compliance Rate
We computed compliance rate scores for participants. We based this off the valid observation definition above and that each participant should have 18 valid observations.
```{r}
obsno_max = 18
study_sample$compliance = ave(study_sample$valid, study_sample$participant_id, FUN=function(x) sum(x, na.rm=TRUE)) / obsno_max

study_sample %>%
    group_by(participant_id) %>% slice(1) %>%    # Keep one row per participant
    ggplot(aes(y=compliance, x=factor(participant_id))) +
        geom_col(position = "dodge") +
        labs(title="Compliance score of each participant",
            y="Compliance",x="Participant id")
study_sample %>%
    group_by(participant_id) %>% slice(1) %>%    # Keep one row per participant
    ggplot(aes(x=compliance)) +
        geom_histogram() +
        labs(title="Histogram of the compliance score",
            y="Number of participant",x="Compliance")
```

# Step 4: Compute and transform variables

This section is dedicated to computing and modifying variables of interest that will later be used in visualization and statistical analysis.

# Modification: Person-level mean and person-mean-centered versions of variables that will be used in analyses.`r txt('esm-mod','Modification',"the 'pos_aff' and 'neg_aff' variables are person-mean centered.")` 
```{r}
# Variables to process
vars <- c("mood", "smoke", "unpleasant", "prior_mood", "pleasant")

study_sample <- study_sample %>%
  group_by(participant_id) %>%  # compute within each person
  mutate(
    across(all_of(vars), ~ mean(.x, na.rm = TRUE), .names = "{.col}_pm"),  # person mean (between-person)
    across(all_of(vars), ~ .x - mean(.x, na.rm = TRUE), .names = "{.col}_pmc")  # person-mean centered (within-person)
  ) %>%
  ungroup()
```

Preprocessed data meta-info
```{r}
cat("## Path      : in-memory dataframe\n")
cat("## Extension : RDS (original format, now in-memory)\n")
cat("## Size      :", format(object.size(study_sample), units = "auto"), "\n\n")

# Dimensions
cat("## ncol      :", ncol(study_sample), "\n")
cat("## nrow      :", nrow(study_sample), "\n\n")

# Number of participants
id_var <- "participant_id"
n_participants <- n_distinct(study_sample[[id_var]])
cat("## Number of participants :", n_participants, "\n")

# Average number of observations per participant
avg_obs <- nrow(study_sample) / n_participants
cat("## Average number of observations per participant :", round(avg_obs, 1), "\n\n")

# Time period
time_var <- "init_time"
if(time_var %in% names(study_sample)) {
  time_min <- min(study_sample[[time_var]], na.rm = TRUE)
  time_max <- max(study_sample[[time_var]], na.rm = TRUE)
  cat("## Period : from", format(time_min, "%Y-%m-%d %H:%M:%S"),
      "to", format(time_max, "%Y-%m-%d %H:%M:%S"), "\n\n")
} else {
  cat("## Period : init_time variable not found in dataset\n\n")
}

# Variable names
cat("## Variables :", paste(names(study_sample), collapse = ", "), "\n")
```
```{r}
# Save the study_sample dataframe as an RDS file
saveRDS(study_sample, file = here("output", "cleaned_study_sample.RDS"))
```

#Revol, J., Carlier, C., Lafit, G., Verhees, M., Sels, L., & Ceulemans, E. (2024). Preprocessing ESM data: a step-by-step framework, tutorial website, R package, and reporting templates.

